{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85c2b6e6-c454-47d8-a719-db3102b35884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb6152df-54a0-4eab-a561-6fb68f54d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3928e852-bdbe-4b6b-a92d-86552aab4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89fcd3a0-a0ed-4e68-baa6-865923c3e05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/113202839.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Attendance (%)\"].fillna(train_df[\"Attendance (%)\"].median(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/113202839.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Assignments_Avg\"].fillna(train_df[\"Assignments_Avg\"].median(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/113202839.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train_df[\"Attendance (%)\"].fillna(train_df[\"Attendance (%)\"].median(), inplace=True)\n",
    "train_df[\"Assignments_Avg\"].fillna(train_df[\"Assignments_Avg\"].median(), inplace=True)\n",
    "train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7b42ede-9391-4d4c-8368-694f6b97e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2080846191.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"Attendance (%)\"].fillna(test_df[\"Attendance (%)\"].median(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2080846191.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"Assignments_Avg\"].fillna(test_df[\"Assignments_Avg\"].median(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2080846191.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "test_df[\"Attendance (%)\"].fillna(test_df[\"Attendance (%)\"].median(), inplace=True)\n",
    "test_df[\"Assignments_Avg\"].fillna(test_df[\"Assignments_Avg\"].median(), inplace=True)\n",
    "test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a006d79-3fc0-48d4-9524-1c64cef30478",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = test_df[\"Student_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e1573b1-702d-4c4e-99f9-53650592503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=[\"Student_ID\",\"First_Name\", \"Last_Name\", \"Email\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "321f8de0-6bca-4a1c-bf8a-5ac33ca2f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.drop(columns=[\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20b75513-50f7-45a6-8329-905e2b3d25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=[\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"], drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=[\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9430b74-dff4-4feb-8e5f-b2101201eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referensi chat gpt\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],  # Urutan pendidikan orang tua\n",
    "    [\"Low\", \"Medium\", \"High\"]  # Urutan tingkat pendapatan keluarga\n",
    "]\n",
    "\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f9a3bf8-2df4-426e-bc82-15d826132170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentukan kolom yang ada di test_df\n",
    "ordinal_cols_test = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings_test = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],  # Parent Education Level\n",
    "    [\"Low\", \"Medium\", \"High\"]  # Family Income Level\n",
    "]\n",
    "\n",
    "encoder_test = OrdinalEncoder(categories=ordinal_mappings_test)\n",
    "test_df[ordinal_cols_test] = encoder_test.fit_transform(test_df[ordinal_cols_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f87eaa80-e761-40ed-9ce0-8c3cbdd50f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"Grade\"])  # Semua fitur kecuali Grade\n",
    "y_train = train_df[\"Grade\"]  # Label\n",
    "\n",
    "X_test = test_df  # Semua fitur dari test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b395c25d-41d3-465d-94ef-b64f4f66cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop kolom yang tidak diperlukan\n",
    "drop_columns = [\"Grade\"]\n",
    "X = train_df.drop(columns=drop_columns)  \n",
    "y = train_df[\"Grade\"]\n",
    "\n",
    "# Split data: 80% train, 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8853c12e-3602-422f-ad7c-f1e973cc06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(prediction, y_test):\n",
    "  accuracy = accuracy_score(y_test, prediction)\n",
    "  f1 = f1_score(y_test, prediction, average=\"macro\")\n",
    "  recall = recall_score(y_test, prediction, average=\"macro\")\n",
    "  precision = precision_score(y_test, prediction, average=\"macro\")\n",
    "\n",
    "  print('Accuracy: ' + str(accuracy))\n",
    "  print('F1 Score: ' + str(f1))\n",
    "  print('Recall Score: ' + str(recall))\n",
    "  print('Precision Score: ' + str(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45ad366e-0836-452d-b13e-8d827256f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30793650793650795\n",
      "F1 Score: 0.30290770403485745\n",
      "Recall Score: 0.3094641526506281\n",
      "Precision Score: 0.30734475509174564\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediksi pada validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluasi model\n",
    "classification_metrics(y_val_pred_rf, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49103a17-d5ed-4b2a-89ea-76717483688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Hyperparameter Tuning:\n",
      "----------------------------------------\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "Best parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best cross-validation score: 0.3181\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Hyperparameter Tuning:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "dt_param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "dt_grid = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=dt_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"Best parameters: {dt_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {dt_grid.best_score_:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "# # Evaluate on validation set\n",
    "# print(\"\\nBest Decision Tree performance on validation set:\")\n",
    "# y_val_pred_best_dt = best_dt.predict(X_val)\n",
    "# dt_best_metrics = classification_metrics(y_val_pred_best_dt, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b727016-d94e-4ffb-8871-683bfa2ea4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Best parameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Best cross-validation score: 0.3333\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Get best model\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# print(\"\\nBest Random Forest performance on validation set:\")\n",
    "# y_val_pred_best_rf = best_rf.predict(X_val)\n",
    "# rf_best_metrics = classification_metrics(y_val_pred_best_rf, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4f4b7ec5-7b92-4f82-8735-b27d48892eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2899876357.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2899876357.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2899876357.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(\"None\", inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2899876357.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(\"None\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.30952\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 🔹 Load dataset\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# 🔹 Feature Engineering\n",
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "\n",
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)\n",
    "\n",
    "train_df[\"Attendance_Study_Ratio\"] = train_df[\"Attendance (%)\"] / (train_df[\"Study_Hours_per_Week\"] + 1)\n",
    "test_df[\"Attendance_Study_Ratio\"] = test_df[\"Attendance (%)\"] / (test_df[\"Study_Hours_per_Week\"] + 1)\n",
    "\n",
    "# 🔹 Handle Missing Values (Numerik)\n",
    "for col in train_df.select_dtypes(include=[\"int64\", \"float64\"]).columns:\n",
    "    train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
    "    test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
    "\n",
    "# 🔹 Drop ID & Irrelevant Columns\n",
    "drop_columns = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
    "train_df.drop(columns=drop_columns, inplace=True)\n",
    "test_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# 🔹 One-Hot Encoding (Kategori Tanpa Urutan)\n",
    "categorical_cols = [\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"]\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 🔹 Ordinal Encoding (Kategori dengan Urutan)\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],\n",
    "    [\"Low\", \"Medium\", \"High\"]\n",
    "]\n",
    "\n",
    "for col in ordinal_cols:\n",
    "    train_df[col].fillna(\"None\", inplace=True)\n",
    "    test_df[col].fillna(\"None\", inplace=True)\n",
    "\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = encoder.transform(test_df[ordinal_cols])\n",
    "\n",
    "# 🔹 Pastikan Semua Kolom Sama di Test\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns) - {\"Grade\"}\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  \n",
    "\n",
    "# 🔹 Split Data\n",
    "X = train_df.drop(columns=[\"Grade\"])\n",
    "y = train_df[\"Grade\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 🔹 Hyperparameter Tuning untuk Decision Tree\n",
    "param_grid = {\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# 🔹 Evaluasi Akurasi\n",
    "val_acc = best_dt.score(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# 🔹 Prediksi Test Set\n",
    "y_pred_test = best_dt.predict(test_df)\n",
    "\n",
    "# 🔹 Simpan Output\n",
    "# submission = pd.DataFrame({\"Student_ID\": test_df.index, \"Grade\": y_pred_test})\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"✅ Prediksi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2c55183-5bd0-46eb-a817-21d66590a9db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Attendance_Study_Ratio\n- Avg_Academic_Score\n- Department_CS\n- Department_Engineering\n- Department_Mathematics\n- ...\nFeature names seen at fit time, yet now missing:\n- Department\n- Extracurricular_Activities\n- First_Name\n- Gender\n- Internet_Access_at_Home\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Melakukan prediksi dengan masing-masing model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# y_pred_best_rf = best_rf.predict(X_test)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_pred_best_dt \u001b[38;5;241m=\u001b[39m best_dt\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m----> 7\u001b[0m y_pred_best_rf \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load sample submission untuk format yang benar\u001b[39;00m\n\u001b[1;32m     11\u001b[0m submission_format \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:904\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    884\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:946\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    949\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:638\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/validation.py:2919\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2836\u001b[0m     _estimator,\n\u001b[1;32m   2837\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2844\u001b[0m ):\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2846\u001b[0m \n\u001b[1;32m   2847\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2918\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2919\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2920\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/validation.py:2777\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2775\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2777\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Attendance_Study_Ratio\n- Avg_Academic_Score\n- Department_CS\n- Department_Engineering\n- Department_Mathematics\n- ...\nFeature names seen at fit time, yet now missing:\n- Department\n- Extracurricular_Activities\n- First_Name\n- Gender\n- Internet_Access_at_Home\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Pastikan test_df sudah tersedia\n",
    "X_test = test_df  # Drop fitur yang tidak dibutuhkan\n",
    "\n",
    "# Melakukan prediksi dengan masing-masing model\n",
    "# y_pred_best_rf = best_rf.predict(X_test)\n",
    "y_pred_best_dt = best_dt.predict(X_test)\n",
    "y_pred_best_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "# Load sample submission untuk format yang benar\n",
    "submission_format = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# Fungsi untuk membuat submission file\n",
    "def create_submission(filename, student_ids, predictions):\n",
    "    submission = submission_format.copy()  # Salin template\n",
    "    submission[\"Student_ID\"] = student_ids\n",
    "    submission[\"Grade\"] = predictions\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Submission saved: {filename}\")\n",
    "\n",
    "# Simpan ke CSV masing-masing model\n",
    "create_submission(\"ScudettoCianoSyam_best_rf2.csv\", test_ids, y_pred_best_rf)\n",
    "create_submission(\"ScudettoCianoSyam_best_dt.csv\", test_ids, y_pred_best_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93c7dbf4-7c82-4097-a885-2e10576a6ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2849848595.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2849848595.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Validation Accuracy: 0.3619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Load dataset\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Feature Engineering\n",
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "\n",
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)\n",
    "\n",
    "train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
    "test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "test_ids = test_df[\"Student_ID\"]\n",
    "drop_columns = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
    "train_df.drop(columns=drop_columns, inplace=True)\n",
    "test_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# Encoding categorical features\n",
    "categorical_cols = [\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"]\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],\n",
    "    [\"Low\", \"Medium\", \"High\"]\n",
    "]\n",
    "\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = encoder.transform(test_df[ordinal_cols])\n",
    "\n",
    "# Ensure test_df has same columns as train_df\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns) - {\"Grade\"}\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0\n",
    "\n",
    "# Split train dataset into train and validation set\n",
    "X = train_df.drop(columns=[\"Grade\"])\n",
    "y = train_df[\"Grade\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define hyperparameters for GridSearch\n",
    "params = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                           params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "accuracy = (y_val_pred == y_val).mean()\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = best_rf.predict(test_df)\n",
    "\n",
    "# # Save predictions\n",
    "# submission = pd.DataFrame({\"Student_ID\": test_df.index, \"Grade\": y_test_pred})\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0237362a-9bd0-466c-af32-80d59b920262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/1527269441.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/1527269441.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/1527269441.py:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/1527269441.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
      "/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3773\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 🔹 Load Data\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# 🔹 Feature Engineering\n",
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "\n",
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)\n",
    "\n",
    "train_df[\"Attendance_Academic_Score\"] = train_df[\"Attendance (%)\"] * train_df[\"Avg_Academic_Score\"]\n",
    "test_df[\"Attendance_Academic_Score\"] = test_df[\"Attendance (%)\"] * test_df[\"Avg_Academic_Score\"]\n",
    "\n",
    "# 🔹 Isi missing values untuk numerik dengan mean\n",
    "for col in train_df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
    "    test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
    "\n",
    "# 🔹 Isi missing values untuk kategorikal dengan mode (nilai yang paling sering muncul)\n",
    "# for col in train_df.select_dtypes(include=['object']).columns:\n",
    "#     train_df[col].fillna(train_df[col].mode()[0], inplace=True)\n",
    "#     test_df[col].fillna(test_df[col].mode()[0], inplace=True)\n",
    "\n",
    "train_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
    "test_df[\"Parent_Education_Level\"].fillna(\"None\", inplace=True)\n",
    "\n",
    "# 🔹 Drop Unused Columns\n",
    "drop_columns = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
    "train_df.drop(columns=drop_columns, inplace=True)\n",
    "test_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# 🔹 Encoding Categorical Features\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],\n",
    "    [\"Low\", \"Medium\", \"High\"]\n",
    "]\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = encoder.transform(test_df[ordinal_cols])\n",
    "\n",
    "categorical_cols = [\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"]\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 🔹 Ensure Test Data Matches Train Columns\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns) - {\"Grade\"}\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0\n",
    "\n",
    "# 🔹 Scaling Numeric Features\n",
    "scaler = StandardScaler()\n",
    "num_cols = [\"Study_Hours_per_Week\", \"Avg_Academic_Score\", \"Effective_Study_Hours\", \"Attendance (%)\", \"Attendance_Academic_Score\"]\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "# 🔹 Define Features & Target\n",
    "X = train_df.drop(columns=[\"Grade\"])\n",
    "y = train_df[\"Grade\"]\n",
    "\n",
    "# 🔹 Handle Imbalanced Data (SMOTE)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# 🔹 Train-Test Split (80% Train, 20% Validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# 🔹 Hyperparameter Tuning (Random Forest)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 300, 400],\n",
    "    \"max_depth\": [10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 🔹 Evaluasi Model pada Validation Set\n",
    "y_val_pred = best_rf.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# 🔹 Prediksi pada Test Set\n",
    "y_test_pred = best_rf.predict(test_df)\n",
    "\n",
    "# 🔹 Simpan Hasil Prediksi\n",
    "# submission = pd.DataFrame({\"Student_ID\": test_df.index, \"Predicted_Grade\": y_test_pred})\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"Prediksi test_df disimpan dalam submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6da9044e-279e-4cd1-b41e-fb9dcb6a83eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2663352325.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2663352325.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2663352325.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(\"None\", inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/2663352325.py:46: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(\"None\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.35238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 🔹 Load dataset\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# 🔹 Feature Engineering\n",
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "\n",
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)\n",
    "\n",
    "train_df[\"Attendance_Study_Ratio\"] = train_df[\"Attendance (%)\"] / (train_df[\"Study_Hours_per_Week\"] + 1)\n",
    "test_df[\"Attendance_Study_Ratio\"] = test_df[\"Attendance (%)\"] / (test_df[\"Study_Hours_per_Week\"] + 1)\n",
    "\n",
    "# 🔹 Handle Missing Values (Hanya Numerik)\n",
    "for col in train_df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
    "    test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
    "\n",
    "# 🔹 Drop ID & Irrelevant Columns\n",
    "drop_columns = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
    "train_df.drop(columns=drop_columns, inplace=True)\n",
    "test_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# 🔹 One-Hot Encoding (Kategori Tanpa Urutan)\n",
    "categorical_cols = [\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"]\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 🔹 Ordinal Encoding (Kategori dengan Urutan)\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],\n",
    "    [\"Low\", \"Medium\", \"High\"]\n",
    "]\n",
    "\n",
    "# 🔹 **Solusi Error: Isi NaN di Kolom Ordinal Sebelum Encoding**\n",
    "for col in ordinal_cols:\n",
    "    train_df[col].fillna(\"None\", inplace=True)\n",
    "    test_df[col].fillna(\"None\", inplace=True)\n",
    "\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = encoder.transform(test_df[ordinal_cols])\n",
    "\n",
    "# 🔹 Pastikan Semua Kolom Sama di Test\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns) - {\"Grade\"}\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  \n",
    "\n",
    "# 🔹 Split Data\n",
    "X = train_df.drop(columns=[\"Grade\"])\n",
    "y = train_df[\"Grade\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 🔹 Model Random Forest dengan Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 🔹 Evaluasi Akurasi\n",
    "val_acc = best_rf.score(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# 🔹 Prediksi Test Set\n",
    "y_pred_test = best_rf.predict(test_df)\n",
    "\n",
    "# 🔹 Simpan Output\n",
    "# submission = pd.DataFrame({\"Student_ID\": test_df.index, \"Grade\": y_pred_test})\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"✅ Prediksi berhasil disimpan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8eb27617-37c4-47a0-815f-563194da4ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.37143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 🔹 Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# 🔹 Simpan Student_ID dari test set (agar tidak hilang)\n",
    "student_id = df_test[\"Student_ID\"]\n",
    "\n",
    "# 🔹 Handle missing values (menggunakan Mean & Mode)\n",
    "def fill_null_mean(df, col):\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "def fill_null_mode(df, col):\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "fill_null_mean(df_train, \"Attendance (%)\")\n",
    "fill_null_mean(df_train, \"Assignments_Avg\")\n",
    "fill_null_mode(df_train, \"Parent_Education_Level\")\n",
    "\n",
    "fill_null_mean(df_test, \"Attendance (%)\")\n",
    "fill_null_mean(df_test, \"Assignments_Avg\")\n",
    "fill_null_mode(df_test, \"Parent_Education_Level\")\n",
    "\n",
    "# 🔹 Drop kolom yang tidak berguna sebelum encoding\n",
    "df_train.drop(columns=[\"Email\", \"Student_ID\"], inplace=True)\n",
    "df_test.drop(columns=[\"Email\", \"Student_ID\"], inplace=True)\n",
    "\n",
    "# 🔹 Encode semua kolom kategori kecuali 'Grade'\n",
    "categorical_columns = df_train.select_dtypes(include=[\"object\"]).columns\n",
    "categorical_columns = categorical_columns[categorical_columns != \"Grade\"]  # Hindari mengubah Grade\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])  # Encode di train\n",
    "\n",
    "    # Transform di test, tapi hindari error jika ada kategori baru\n",
    "    df_test[col] = df_test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else np.nan)\n",
    "\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 🔹 Split data menjadi train & validation set\n",
    "X = df_train.drop(columns=[\"Grade\"])\n",
    "y = df_train[\"Grade\"]  # Biarkan Grade tetap A/B/C\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 🔹 Model Random Forest dengan Hyperparameter Optimal\n",
    "rf = RandomForestClassifier(n_estimators=300, max_depth=12, min_samples_split=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 🔹 Evaluasi Model (Akurasi Validation)\n",
    "val_acc = rf.score(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# 🔹 Prediksi Test Set\n",
    "y_pred_test = rf.predict(df_test)\n",
    "\n",
    "# 🔹 Simpan Output ke CSV\n",
    "# df_submission = pd.DataFrame({\"Student_ID\": student_id, \"Grade\": y_pred_test})\n",
    "# df_submission.to_csv(\"submission.csv\", index=False)\n",
    "# print(\"✅ Prediksi berhasil disimpan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3729c8f8-c4ac-4e57-8511-6429bd12bd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/4253357343.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/4253357343.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/4253357343.py:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(\"None\", inplace=True)\n",
      "/var/folders/vs/zksk8xgx1vxczt_hfgx08gzc0000gn/T/ipykernel_34674/4253357343.py:49: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(\"None\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.30952\n",
      "✅ Prediksi berhasil disimpan!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 🔹 Load dataset\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# 🔹 Simpan Student_ID dari test set\n",
    "student_id = test_df[\"Student_ID\"].copy()\n",
    "\n",
    "# 🔹 Feature Engineering\n",
    "train_df[\"Avg_Academic_Score\"] = train_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "test_df[\"Avg_Academic_Score\"] = test_df[[\"Midterm_Score\", \"Final_Score\", \"Assignments_Avg\", \"Quizzes_Avg\", \"Projects_Score\"]].mean(axis=1)\n",
    "\n",
    "train_df[\"Effective_Study_Hours\"] = train_df[\"Study_Hours_per_Week\"] / (train_df[\"Stress_Level (1-10)\"] + 1)\n",
    "test_df[\"Effective_Study_Hours\"] = test_df[\"Study_Hours_per_Week\"] / (test_df[\"Stress_Level (1-10)\"] + 1)\n",
    "\n",
    "train_df[\"Attendance_Study_Ratio\"] = train_df[\"Attendance (%)\"] / (train_df[\"Study_Hours_per_Week\"] + 1)\n",
    "test_df[\"Attendance_Study_Ratio\"] = test_df[\"Attendance (%)\"] / (test_df[\"Study_Hours_per_Week\"] + 1)\n",
    "\n",
    "# 🔹 Handle Missing Values (Numerik dengan Mean)\n",
    "for col in train_df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    train_df[col].fillna(train_df[col].mean(), inplace=True)\n",
    "    test_df[col].fillna(test_df[col].mean(), inplace=True)\n",
    "\n",
    "# 🔹 Drop ID & Irrelevant Columns\n",
    "drop_columns = [\"Student_ID\", \"First_Name\", \"Last_Name\", \"Email\"]\n",
    "train_df.drop(columns=drop_columns, inplace=True)\n",
    "test_df.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "# 🔹 One-Hot Encoding (Kategori Tanpa Urutan)\n",
    "categorical_cols = [\"Gender\", \"Department\", \"Extracurricular_Activities\", \"Internet_Access_at_Home\"]\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 🔹 Ordinal Encoding (Kategori dengan Urutan)\n",
    "ordinal_cols = [\"Parent_Education_Level\", \"Family_Income_Level\"]\n",
    "ordinal_mappings = [\n",
    "    [\"None\", \"High School\", \"Bachelor's\", \"Master's\", \"PhD\"],\n",
    "    [\"Low\", \"Medium\", \"High\"]\n",
    "]\n",
    "\n",
    "# Isi NaN sebelum encoding\n",
    "for col in ordinal_cols:\n",
    "    train_df[col].fillna(\"None\", inplace=True)\n",
    "    test_df[col].fillna(\"None\", inplace=True)\n",
    "\n",
    "encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "train_df[ordinal_cols] = encoder.fit_transform(train_df[ordinal_cols])\n",
    "test_df[ordinal_cols] = encoder.transform(test_df[ordinal_cols])\n",
    "\n",
    "# 🔹 Pastikan Semua Kolom Sama di Test\n",
    "missing_cols = set(train_df.columns) - set(test_df.columns) - {\"Grade\"}\n",
    "for col in missing_cols:\n",
    "    test_df[col] = 0  \n",
    "\n",
    "# 🔹 Split Data\n",
    "X = train_df.drop(columns=[\"Grade\"])\n",
    "y = train_df[\"Grade\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 🔹 Model Decision Tree dengan Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 10, 15, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_dtc = grid_search.best_estimator_\n",
    "\n",
    "# 🔹 Evaluasi Akurasi\n",
    "val_acc = best_dtc.score(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_acc:.5f}\")\n",
    "\n",
    "# 🔹 Prediksi Test Set\n",
    "y_pred_test = best_dtc.predict(test_df)\n",
    "\n",
    "# 🔹 Simpan Output ke CSV\n",
    "df_submission = pd.DataFrame({\"Student_ID\": student_id, \"Grade\": y_pred_test})\n",
    "df_submission.to_csv(\"sample_submission.csv\", index=False)\n",
    "print(\"✅ Prediksi berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7ddbd96-1a3c-4ee6-a5ea-f9975555274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 20}\n",
      "✅ Submission file saved as sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Method handle null\n",
    "def fill_null_mean(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def fill_null_mode(df):\n",
    "    return df.fillna(df.mode()[0])\n",
    "\n",
    "# Handle null for train_data.csv\n",
    "df_train['Attendance (%)'] = fill_null_mean(df_train['Attendance (%)'])\n",
    "df_train['Assignments_Avg'] = fill_null_mean(df_train['Assignments_Avg'])\n",
    "df_train['Parent_Education_Level'] = fill_null_mode(df_train['Parent_Education_Level'])\n",
    "\n",
    "# Handle null for test_data.csv\n",
    "df_test['Attendance (%)'] = fill_null_mean(df_test['Attendance (%)'])\n",
    "df_test['Assignments_Avg'] = fill_null_mean(df_test['Assignments_Avg'])\n",
    "df_test['Parent_Education_Level'] = fill_null_mode(df_test['Parent_Education_Level'])\n",
    "\n",
    "# Drop unique columns\n",
    "df_train.drop(columns=['Email'], inplace=True)\n",
    "df_test.drop(columns=['Email'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Splitting train and test from dataset train_data.csv\n",
    "X = df_train.drop(columns=['Student_ID', 'Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardize numerical columns\n",
    "numerical_columns = ['Attendance (%)', 'Assignments_Avg']\n",
    "scaler = StandardScaler()\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
    "\n",
    "# Hyperparameter tuning for Random Forest Classifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "clf = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, n_iter=20, cv=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Train Random Forest Classifier with best parameters\n",
    "rfc_best = RandomForestClassifier(\n",
    "    n_estimators=clf.best_params_['n_estimators'],\n",
    "    max_depth=clf.best_params_['max_depth'],\n",
    "    min_samples_split=clf.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=clf.best_params_['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "rfc_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction_kag = rfc_best.predict(df_test)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = student_id\n",
    "df_submission['Grade'] = prediction_kag\n",
    "df_submission.to_csv(\"submit4.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aff625bc-c4be-45d7-b2b6-d73581f4b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'min_samples_split': 50, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 5, 'criterion': 'gini'}\n",
      "✅ Submission file saved as sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Method handle null\n",
    "def fill_null_mean(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def fill_null_median(df):\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "def fill_null_mode(df):\n",
    "    return df.fillna(df.mode()[0])\n",
    "\n",
    "# Handle null for train_data.csv\n",
    "df_train['Attendance (%)'] = fill_null_mean(df_train['Attendance (%)'])\n",
    "df_train['Assignments_Avg'] = fill_null_mean(df_train['Assignments_Avg'])\n",
    "df_train['Parent_Education_Level'] = fill_null_mode(df_train['Parent_Education_Level'])\n",
    "\n",
    "# Handle null for test_data.csv\n",
    "df_test['Attendance (%)'] = fill_null_mean(df_test['Attendance (%)'])\n",
    "df_test['Assignments_Avg'] = fill_null_mean(df_test['Assignments_Avg'])\n",
    "df_test['Parent_Education_Level'] = fill_null_mode(df_test['Parent_Education_Level'])\n",
    "\n",
    "# Drop unique columns\n",
    "df_train.drop(columns=['Email'], inplace=True)\n",
    "df_test.drop(columns=['Email'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Splitting train and test from dataset train_data.csv\n",
    "X = df_train.drop(columns=['Student_ID', 'Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {cls: weight for cls, weight in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree Classifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [5, 10, 20, 50],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dtc_kag = DecisionTreeClassifier(class_weight=class_weight_dict, random_state=42)\n",
    "clf_kag = RandomizedSearchCV(estimator=dtc_kag, param_distributions=param_grid, cv=5, n_iter=20, random_state=42)\n",
    "clf_kag.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", clf_kag.best_params_)\n",
    "\n",
    "# Train Decision Tree Classifier with best parameters\n",
    "dtc_kag1 = DecisionTreeClassifier(\n",
    "    criterion=clf_kag.best_params_['criterion'],\n",
    "    max_depth=clf_kag.best_params_['max_depth'],\n",
    "    min_samples_split=clf_kag.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=clf_kag.best_params_['min_samples_leaf'],\n",
    "    max_features=clf_kag.best_params_['max_features'],\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42\n",
    ")\n",
    "dtc_kag1.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction_kag = dtc_kag1.predict(df_test)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = student_id\n",
    "df_submission['Grade'] = prediction_kag\n",
    "df_submission.to_csv(\"submit5.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "713d0b49-b045-4f60-80c7-a63d7cd3beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 5}\n",
      "✅ Submission file saved as sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values using mean for numerical and mode for categorical\n",
    "def fill_null_mean(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "def fill_null_mode(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "num_cols = ['Attendance (%)', 'Assignments_Avg']\n",
    "cat_cols = ['Parent_Education_Level']\n",
    "\n",
    "df_train = fill_null_mean(df_train, num_cols)\n",
    "df_train = fill_null_mode(df_train, cat_cols)\n",
    "\n",
    "df_test = fill_null_mean(df_test, num_cols)\n",
    "df_test = fill_null_mode(df_test, cat_cols)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = ['Email', 'Student_ID']\n",
    "student_id = df_test['Student_ID']\n",
    "df_train.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "df_test.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "le = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "\n",
    "# Split dataset\n",
    "X = df_train.drop(columns=['Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "clf = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Train Random Forest with best parameters\n",
    "rf_best = RandomForestClassifier(**clf.best_params_, random_state=42)\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction = rf_best.predict(df_test)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': prediction})\n",
    "df_submission.to_csv(\"submit6.csv\", index=False)\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec121ebd-0bb4-4cba-a308-9eb43ad89926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5}\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "✅ Submission file saved as sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values with SimpleImputer\n",
    "imputer_mean = SimpleImputer(strategy=\"mean\")\n",
    "imputer_mode = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "numerical_cols = ['Attendance (%)', 'Assignments_Avg']\n",
    "categorical_cols = ['Parent_Education_Level']\n",
    "\n",
    "df_train[numerical_cols] = imputer_mean.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = imputer_mean.transform(df_test[numerical_cols])\n",
    "df_train[categorical_cols] = imputer_mode.fit_transform(df_train[categorical_cols])\n",
    "df_test[categorical_cols] = imputer_mode.transform(df_test[categorical_cols])\n",
    "\n",
    "# Drop unique columns\n",
    "df_train.drop(columns=['Email'], inplace=True)\n",
    "df_test.drop(columns=['Email'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Feature Selection using Random Forest Importance\n",
    "X = df_train.drop(columns=['Student_ID', 'Grade'])\n",
    "y = df_train['Grade']\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X, y)\n",
    "feature_importances = pd.Series(rf_selector.feature_importances_, index=X.columns)\n",
    "selected_features = feature_importances.nlargest(10).index.tolist()\n",
    "X = X[selected_features]\n",
    "df_test = df_test[selected_features]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
    "\n",
    "# Splitting train and test from dataset train_data.csv\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree Classifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "clf_dtc = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5)\n",
    "clf_dtc.fit(X_train, y_train)\n",
    "print(\"Best parameters for Decision Tree:\", clf_dtc.best_params_)\n",
    "\n",
    "dtc_best = DecisionTreeClassifier(**clf_dtc.best_params_, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [5, 10]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "clf_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"Best parameters for Random Forest:\", clf_rf.best_params_)\n",
    "\n",
    "rf_best = RandomForestClassifier(**clf_rf.best_params_, random_state=42)\n",
    "\n",
    "# Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=[('dt', dtc_best), ('rf', rf_best)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction = voting_clf.predict(df_test)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = student_id\n",
    "df_submission['Grade'] = prediction\n",
    "df_submission.to_csv(\"submit7.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a0b0b977-df4a-4e77-85dc-9dfa6c837162",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Grade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Grade'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m X \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudent_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGrade\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGrade\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     50\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Hyperparameter tuning\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Grade'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"df_sample.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "def fill_null_mean(df, column):\n",
    "    return df[column].fillna(df[column].mean())\n",
    "\n",
    "def fill_null_mode(df, column):\n",
    "    return df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "numerical_cols = ['Attendance (%)', 'Assignments_Avg']\n",
    "categorical_cols = ['Parent_Education_Level']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    df_train[col] = fill_null_mean(df_train, col)\n",
    "    df_test[col] = fill_null_mean(df_test, col)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_train[col] = fill_null_mode(df_train, col)\n",
    "    df_test[col] = fill_null_mode(df_test, col)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_train.drop(columns=['Email'], inplace=True, errors='ignore')\n",
    "df_test.drop(columns=['Email'], inplace=True, errors='ignore')\n",
    "\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Student_ID'], inplace=True, errors='ignore')\n",
    "\n",
    "# Encode categorical columns\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = le.fit_transform(df_train[col])\n",
    "        df_test[col] = le.transform(df_test[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Split dataset\n",
    "X = df_train.drop(columns=['Student_ID', 'Grade'], errors='ignore')\n",
    "y = df_train['Grade']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "clf = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Train model with best parameters\n",
    "dtc_best = DecisionTreeClassifier(**clf.best_params_, random_state=42)\n",
    "dtc_best.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction = dtc_best.predict(df_test)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': prediction})\n",
    "df_submission.to_csv(\"/mnt/data/sample_submission.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "46b257f6-d21e-40ca-b496-dc5de54073fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [\"Master's\", 'PhD', \"Bachelor's\"] in column 0 during fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m X_test_final \u001b[38;5;241m=\u001b[39m df_test\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Apply transformation\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m X_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mdefine_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m X_test_transformed \u001b[38;5;241m=\u001b[39m define_transformer\u001b[38;5;241m.\u001b[39mtransform(X_test_final)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1001\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m-> 1001\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:910\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    898\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    899\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    900\u001b[0m             delayed(func)(\n\u001b[1;32m    901\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m             )\n\u001b[1;32m    908\u001b[0m         )\n\u001b[0;32m--> 910\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py:1551\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1551\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1554\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1555\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:1515\u001b[0m, in \u001b[0;36mOrdinalEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown_value should only be set when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle_unknown is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munknown_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m     )\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# `_fit` will only raise an error when `self.handle_unknown=\"error\"`\u001b[39;00m\n\u001b[0;32m-> 1515\u001b[0m fit_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_and_ignore_missing_for_infrequent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_missing_indices \u001b[38;5;241m=\u001b[39m fit_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1523\u001b[0m cardinalities \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(categories) \u001b[38;5;28;01mfor\u001b[39;00m categories \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:164\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, ensure_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m    160\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m in column \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m during fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(diff, i)\n\u001b[1;32m    163\u001b[0m         )\n\u001b[0;32m--> 164\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_counts:\n\u001b[1;32m    166\u001b[0m     category_counts\u001b[38;5;241m.\u001b[39mappend(_get_counts(Xi, cats))\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [\"Master's\", 'PhD', \"Bachelor's\"] in column 0 during fit"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "imputer_mean = SimpleImputer(strategy=\"mean\")\n",
    "imputer_mode = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "numerical_cols = ['Attendance (%)', 'Assignments_Avg']\n",
    "categorical_ordinal = ['Parent_Education_Level']  # Contoh ordinal\n",
    "categorical_ohe = ['Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Family_Income_Level']\n",
    "\n",
    "# Impute missing values\n",
    "df_train[numerical_cols] = imputer_mean.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = imputer_mean.transform(df_test[numerical_cols])\n",
    "df_train[categorical_ordinal] = imputer_mode.fit_transform(df_train[categorical_ordinal])\n",
    "df_test[categorical_ordinal] = imputer_mode.transform(df_test[categorical_ordinal])\n",
    "\n",
    "# Drop unique columns\n",
    "df_train.drop(columns=['Email'], inplace=True)\n",
    "df_test.drop(columns=['Email'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Student_ID'], inplace=True)\n",
    "\n",
    "define_ordinal_mapping = [['High School', 'Associate', 'Bachelor', 'Master', 'Doctorate']]\n",
    "\n",
    "define_transformer = ColumnTransformer([\n",
    "    ('ordinal', OrdinalEncoder(categories=define_ordinal_mapping), categorical_ordinal),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'), categorical_ohe)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Prepare features & target\n",
    "X = df_train.drop(columns=['Student_ID', 'Grade'])\n",
    "y = df_train['Grade']\n",
    "X_test_final = df_test\n",
    "\n",
    "# Apply transformation\n",
    "X_transformed = define_transformer.fit_transform(X)\n",
    "X_test_transformed = define_transformer.transform(X_test_final)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "clf_dtc = GridSearchCV(dtc, param_grid, cv=5)\n",
    "clf_dtc.fit(X_train, y_train)\n",
    "print(\"Best Decision Tree parameters:\", clf_dtc.best_params_)\n",
    "\n",
    "# Train best Decision Tree model\n",
    "best_dtc = DecisionTreeClassifier(**clf_dtc.best_params_, random_state=42)\n",
    "best_dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict test data\n",
    "prediction = best_dtc.predict(X_test_transformed)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': prediction})\n",
    "df_submission.to_csv(\"sample_submission.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c8528cba-4f56-469d-adbc-dac13f9deb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Student_ID:\n",
      " Student_ID\n",
      "S1406    1\n",
      "S3727    1\n",
      "S5914    1\n",
      "S2175    1\n",
      "S1470    1\n",
      "        ..\n",
      "S4679    1\n",
      "S5217    1\n",
      "S3838    1\n",
      "S4152    1\n",
      "S3664    1\n",
      "Name: count, Length: 1050, dtype: int64 \n",
      "\n",
      "Unique values in First_Name:\n",
      " First_Name\n",
      "Ahmed    144\n",
      "John     138\n",
      "Emma     135\n",
      "Sara     134\n",
      "Omar     129\n",
      "Ali      129\n",
      "Liam     122\n",
      "Maria    119\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Last_Name:\n",
      " Last_Name\n",
      "Jones       204\n",
      "Davis       195\n",
      "Brown       176\n",
      "Smith       162\n",
      "Williams    158\n",
      "Johnson     155\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Email:\n",
      " Email\n",
      "student406@university.com     1\n",
      "student2727@university.com    1\n",
      "student4914@university.com    1\n",
      "student1175@university.com    1\n",
      "student470@university.com     1\n",
      "                             ..\n",
      "student3679@university.com    1\n",
      "student4217@university.com    1\n",
      "student2838@university.com    1\n",
      "student3152@university.com    1\n",
      "student2664@university.com    1\n",
      "Name: count, Length: 1050, dtype: int64 \n",
      "\n",
      "Unique values in Gender:\n",
      " Gender\n",
      "Male      548\n",
      "Female    502\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Department:\n",
      " Department\n",
      "CS             440\n",
      "Engineering    295\n",
      "Business       210\n",
      "Mathematics    105\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Grade:\n",
      " Grade\n",
      "B    220\n",
      "C    213\n",
      "F    209\n",
      "D    205\n",
      "A    203\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Extracurricular_Activities:\n",
      " Extracurricular_Activities\n",
      "No     736\n",
      "Yes    314\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Internet_Access_at_Home:\n",
      " Internet_Access_at_Home\n",
      "Yes    940\n",
      "No     110\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Parent_Education_Level:\n",
      " Parent_Education_Level\n",
      "PhD            169\n",
      "Master's       168\n",
      "High School    165\n",
      "Bachelor's     165\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Unique values in Family_Income_Level:\n",
      " Family_Income_Level\n",
      "Low       435\n",
      "Medium    404\n",
      "High      211\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "for col in df_train.select_dtypes(include=['object']).columns:\n",
    "    print(f\"Unique values in {col}:\\n\", df_train[col].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b1bbcc9f-2c87-45cc-86f6-2cc719a188c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_split': 5}\n",
      "✅ Submission file saved as submission_decision_tree.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === 1. Load Dataset ===\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# === 2. Drop Unnecessary Columns ===\n",
    "df_train.drop(columns=['Student_ID', 'First_Name', 'Last_Name', 'Email'], inplace=True)\n",
    "df_test.drop(columns=['Student_ID', 'First_Name', 'Last_Name', 'Email'], inplace=True)\n",
    "\n",
    "# === 3. Handle Missing Values ===\n",
    "imputer_mean = SimpleImputer(strategy=\"mean\")  # Untuk nilai numerik\n",
    "imputer_mode = SimpleImputer(strategy=\"most_frequent\")  # Untuk nilai kategorikal\n",
    "\n",
    "numerical_cols = ['Attendance (%)', 'Assignments_Avg']\n",
    "categorical_cols = ['Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Family_Income_Level']\n",
    "\n",
    "df_train[numerical_cols] = imputer_mean.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = imputer_mean.transform(df_test[numerical_cols])\n",
    "\n",
    "df_train[categorical_cols] = imputer_mode.fit_transform(df_train[categorical_cols])\n",
    "df_test[categorical_cols] = imputer_mode.transform(df_test[categorical_cols])\n",
    "\n",
    "# === 4. Encode Categorical Features ===\n",
    "edu_order = [\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"]  # Urutan tingkat pendidikan\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_cols),  # Standardize numerical data\n",
    "    ('edu', OrdinalEncoder(categories=[edu_order], handle_unknown=\"use_encoded_value\", unknown_value=-1), ['Parent_Education_Level']),  # Ordinal Encode\n",
    "    ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)  # One-Hot Encode lainnya\n",
    "])\n",
    "\n",
    "# === 5. Prepare Data for Training ===\n",
    "X = df_train.drop(columns=['Grade'])\n",
    "y = df_train['Grade']\n",
    "\n",
    "# === 6. Split Data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# === 7. Build Decision Tree Model with Pipeline ===\n",
    "dt_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# === 8. Hyperparameter Tuning ===\n",
    "param_grid = {\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 5, 10, 15],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(dt_pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# === 9. Best Model ===\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# === 10. Predict Test Set ===\n",
    "y_pred = best_dt_model.predict(df_test)\n",
    "\n",
    "# === 11. Save Predictions ===\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = pd.read_csv(\"test_data.csv\")['Student_ID']  # Ambil Student_ID kembali\n",
    "df_submission['Grade'] = y_pred\n",
    "df_submission.to_csv(\"submission_decision_tree.csv\", index=False)\n",
    "\n",
    "print(\"✅ Submission file saved as submission_decision_tree.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "88e8987f-a3aa-4aa7-b6a6-7f9c4e3ad4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Function to handle missing values\n",
    "def fill_null_mean(df, col):\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "def fill_null_mode(df, col):\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Handling missing values in training & testing data\n",
    "fill_null_mean(df_train, 'Attendance (%)')\n",
    "fill_null_mean(df_train, 'Assignments_Avg')\n",
    "fill_null_mode(df_train, 'Parent_Education_Level')\n",
    "\n",
    "fill_null_mean(df_test, 'Attendance (%)')\n",
    "fill_null_mean(df_test, 'Assignments_Avg')\n",
    "fill_null_mode(df_test, 'Parent_Education_Level')\n",
    "\n",
    "# Drop unique identifier columns\n",
    "df_train.drop(columns=['Email', 'Student_ID'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Email', 'Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', \n",
    "                       'Extracurricular_Activities', 'Internet_Access_at_Home', \n",
    "                       'Parent_Education_Level', 'Family_Income_Level']\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Splitting dataset\n",
    "X = df_train.drop(columns=['Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Hyperparameter tuning for Decision Tree\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [10, 20, 50],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_params = clf.best_params_\n",
    "dtc = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = dtc.predict(df_test)\n",
    "\n",
    "# Save submission file\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': predictions})\n",
    "df_submission.to_csv(\"submit888.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6c55015-f90a-49f3-b3bb-157499f4698d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 990, in fit_transform\n    self._validate_transformers()\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 530, in _validate_transformers\n    raise TypeError(\nTypeError: All estimators should implement fit and transform, or can be 'drop' or 'passthrough' specifiers. 'Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                ('onehot', <function get_dummies at 0x12e9b9e50>)])' (type <class 'sklearn.pipeline.Pipeline'>) doesn't.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Find the best parameters\u001b[39;00m\n\u001b[1;32m     85\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[1;32m     89\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    999\u001b[0m     )\n\u001b[0;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[0;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 990, in fit_transform\n    self._validate_transformers()\n  File \"/opt/miniconda3/envs/main-ds/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 530, in _validate_transformers\n    raise TypeError(\nTypeError: All estimators should implement fit and transform, or can be 'drop' or 'passthrough' specifiers. 'Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                ('onehot', <function get_dummies at 0x12e9b9e50>)])' (type <class 'sklearn.pipeline.Pipeline'>) doesn't.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Save student IDs for submission\n",
    "student_id = df_test['Student_ID']\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values in numerical columns with median\n",
    "    for col in df_processed.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    # For categorical columns, fill with most frequent value\n",
    "    for col in df_processed.select_dtypes(include=['object']).columns:\n",
    "        if col != 'Student_ID' and df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess the data\n",
    "df_train_processed = preprocess_data(df_train)\n",
    "df_test_processed = preprocess_data(df_test)\n",
    "\n",
    "# Identify target variable\n",
    "target_column = 'Grade'  # Target is now the Grade column\n",
    "y_train = df_train_processed[target_column]\n",
    "\n",
    "# Define features (excluding Student_ID, First_Name, Last_Name, Email and the target)\n",
    "exclude_cols = ['Student_ID', 'First_Name', 'Last_Name', 'Email', target_column]\n",
    "feature_cols = [col for col in df_train_processed.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = df_train_processed[feature_cols]\n",
    "X_test = df_test_processed[feature_cols]\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', pd.get_dummies)\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with preprocessing and the decision tree model\n",
    "# Using a classifier instead of a regressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Simple hyperparameter tuning - keeping the model simple as requested\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__min_samples_split': [5, 10]\n",
    "}\n",
    "\n",
    "# Find the best parameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "prediction = best_model.predict(X_test)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = student_id\n",
    "df_submission['Grade'] = prediction\n",
    "df_submission.to_csv(\"submit7.csv\", index=False)\n",
    "print(\"✅ Submission file saved as submit99.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "454035d2-4bd6-4a85-908a-8f878550b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing data...\n",
      "Using 12 numerical features and 6 categorical features\n",
      "Training the model...\n",
      "Making predictions...\n",
      "✅ Submission file saved as submit7.csv\n",
      "\n",
      "Decision Tree Rules (limited to depth 3):\n",
      "|--- num__Attendance (%) <= 0.85\n",
      "|   |--- num__Attendance (%) <= -0.93\n",
      "|   |   |--- num__Total_Score <= 0.56\n",
      "|   |   |   |--- class: D\n",
      "|   |   |--- num__Total_Score >  0.56\n",
      "|   |   |   |--- class: C\n",
      "|   |--- num__Attendance (%) >  -0.93\n",
      "|   |   |--- cat__Parent_Education_Level_PhD <= 0.50\n",
      "|   |   |   |--- class: B\n",
      "|   |   |--- cat__Parent_Education_Level_PhD >  0.50\n",
      "|   |   |   |--- class: C\n",
      "|--- num__Attendance (%) >  0.85\n",
      "|   |--- num__Total_Score <= 1.17\n",
      "|   |   |--- num__Stress_Level (1-10) <= -1.05\n",
      "|   |   |   |--- class: A\n",
      "|   |   |--- num__Stress_Level (1-10) >  -1.05\n",
      "|   |   |   |--- class: A\n",
      "|   |--- num__Total_Score >  1.17\n",
      "|   |   |--- num__Assignments_Avg <= 0.54\n",
      "|   |   |   |--- class: B\n",
      "|   |   |--- num__Assignments_Avg >  0.54\n",
      "|   |   |   |--- class: A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Save student IDs for submission\n",
    "student_id = df_test['Student_ID']\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values in numerical columns with median\n",
    "    for col in df_processed.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    \n",
    "    # For categorical columns, fill with most frequent value\n",
    "    for col in df_processed.select_dtypes(include=['object']).columns:\n",
    "        if col != 'Student_ID' and col != 'Email' and df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col] = df_processed[col].fillna(df_processed[col].mode()[0])\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "df_train_processed = preprocess_data(df_train)\n",
    "df_test_processed = preprocess_data(df_test)\n",
    "\n",
    "# Identify target variable\n",
    "target_column = 'Grade'\n",
    "y_train = df_train_processed[target_column]\n",
    "\n",
    "# Define features (excluding Student_ID, First_Name, Last_Name, Email and the target)\n",
    "exclude_cols = ['Student_ID', 'First_Name', 'Last_Name', 'Email', target_column]\n",
    "feature_cols = [col for col in df_train_processed.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = df_train_processed[feature_cols]\n",
    "X_test = df_test_processed[feature_cols]\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Using {len(numerical_cols)} numerical features and {len(categorical_cols)} categorical features\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Create a simple decision tree model - keeping it shallow to avoid overfitting\n",
    "# Based on your comment about complex models performing worse\n",
    "model = DecisionTreeClassifier(\n",
    "    max_depth=3,              # Shallow tree to avoid overfitting\n",
    "    min_samples_split=10,     # Require more samples to split to prevent overfitting\n",
    "    min_samples_leaf=5,       # Don't create tiny leaf nodes\n",
    "    random_state=42           # For reproducibility\n",
    ")\n",
    "\n",
    "# Create a pipeline with preprocessing and the decision tree model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the model directly without grid search to keep it simple\n",
    "print(\"Training the model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "prediction = pipeline.predict(X_test)\n",
    "\n",
    "# Save results to CSV\n",
    "df_submission = pd.DataFrame()\n",
    "df_submission['Student_ID'] = student_id\n",
    "df_submission['Grade'] = prediction\n",
    "df_submission.to_csv(\"submit10.csv\", index=False)\n",
    "print(\"✅ Submission file saved as submit7.csv\")\n",
    "\n",
    "# Optional: Print model characteristics\n",
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(pipeline.named_steps['model'], \n",
    "                        feature_names=pipeline.named_steps['preprocessor'].get_feature_names_out().tolist())\n",
    "print(\"\\nDecision Tree Rules (limited to depth 3):\")\n",
    "print(tree_rules[:1000])  # Print part of the tree to avoid overwhelming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9f0bf00e-0d21-4aed-9fc2-9cad78a6ab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/main-ds/lib/python3.9/site-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/main-ds/lib/python3.9/site-packages (from optuna) (24.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.38-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/miniconda3/envs/main-ds/lib/python3.9/site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/miniconda3/envs/main-ds/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/miniconda3/envs/main-ds/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.38-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1 sqlalchemy-2.0.38 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1411d8c1-080e-4b49-8999-3f40890a82f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 21:57:58,862] A new study created in memory with name: no-name-f6aadd10-d8b1-4315-838c-6b6c9b15995d\n",
      "[I 2025-03-02 21:57:58,874] Trial 0 finished with value: 0.3 and parameters: {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_split': 17}. Best is trial 0 with value: 0.3.\n",
      "[I 2025-03-02 21:57:58,882] Trial 1 finished with value: 0.3238095238095238 and parameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 38}. Best is trial 1 with value: 0.3238095238095238.\n",
      "[I 2025-03-02 21:57:58,890] Trial 2 finished with value: 0.37142857142857144 and parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 35}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,899] Trial 3 finished with value: 0.32857142857142857 and parameters: {'criterion': 'log_loss', 'max_depth': 13, 'min_samples_split': 47}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,905] Trial 4 finished with value: 0.319047619047619 and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 72}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,911] Trial 5 finished with value: 0.35714285714285715 and parameters: {'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 50}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,920] Trial 6 finished with value: 0.35714285714285715 and parameters: {'criterion': 'entropy', 'max_depth': 16, 'min_samples_split': 52}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,929] Trial 7 finished with value: 0.34285714285714286 and parameters: {'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 5}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,937] Trial 8 finished with value: 0.3619047619047619 and parameters: {'criterion': 'log_loss', 'max_depth': 20, 'min_samples_split': 49}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,945] Trial 9 finished with value: 0.3238095238095238 and parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 53}. Best is trial 2 with value: 0.37142857142857144.\n",
      "[I 2025-03-02 21:57:58,954] Trial 10 finished with value: 0.38095238095238093 and parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 94}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:58,962] Trial 11 finished with value: 0.37142857142857144 and parameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 96}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:58,970] Trial 12 finished with value: 0.37142857142857144 and parameters: {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 94}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:58,980] Trial 13 finished with value: 0.36666666666666664 and parameters: {'criterion': 'entropy', 'max_depth': 6, 'min_samples_split': 74}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:58,989] Trial 14 finished with value: 0.38095238095238093 and parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 24}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:58,996] Trial 15 finished with value: 0.3523809523809524 and parameters: {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 22}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:59,007] Trial 16 finished with value: 0.3380952380952381 and parameters: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 71}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:59,018] Trial 17 finished with value: 0.35714285714285715 and parameters: {'criterion': 'entropy', 'max_depth': 13, 'min_samples_split': 84}. Best is trial 10 with value: 0.38095238095238093.\n",
      "[I 2025-03-02 21:57:59,027] Trial 18 finished with value: 0.4 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 3}. Best is trial 18 with value: 0.4.\n",
      "[I 2025-03-02 21:57:59,036] Trial 19 finished with value: 0.32857142857142857 and parameters: {'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 2}. Best is trial 18 with value: 0.4.\n",
      "[I 2025-03-02 21:57:59,045] Trial 20 finished with value: 0.3476190476190476 and parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_split': 64}. Best is trial 18 with value: 0.4.\n",
      "[I 2025-03-02 21:57:59,055] Trial 21 finished with value: 0.4095238095238095 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 17}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,064] Trial 22 finished with value: 0.4 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 11}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,073] Trial 23 finished with value: 0.30952380952380953 and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 11}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,084] Trial 24 finished with value: 0.3476190476190476 and parameters: {'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 12}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,095] Trial 25 finished with value: 0.40476190476190477 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 27}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,112] Trial 26 finished with value: 0.3238095238095238 and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 29}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,128] Trial 27 finished with value: 0.36666666666666664 and parameters: {'criterion': 'gini', 'max_depth': 19, 'min_samples_split': 32}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,141] Trial 28 finished with value: 0.3619047619047619 and parameters: {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 20}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,157] Trial 29 finished with value: 0.3619047619047619 and parameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 16}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,172] Trial 30 finished with value: 0.3761904761904762 and parameters: {'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 41}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,182] Trial 31 finished with value: 0.4 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 7}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,193] Trial 32 finished with value: 0.38571428571428573 and parameters: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 15}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,203] Trial 33 finished with value: 0.37142857142857144 and parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_split': 28}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,212] Trial 34 finished with value: 0.4 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 8}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,222] Trial 35 finished with value: 0.37142857142857144 and parameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 41}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,230] Trial 36 finished with value: 0.38571428571428573 and parameters: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 19}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,241] Trial 37 finished with value: 0.3142857142857143 and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 12}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,249] Trial 38 finished with value: 0.3952380952380952 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,262] Trial 39 finished with value: 0.3238095238095238 and parameters: {'criterion': 'log_loss', 'max_depth': 9, 'min_samples_split': 26}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,272] Trial 40 finished with value: 0.38095238095238093 and parameters: {'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 38}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,281] Trial 41 finished with value: 0.4 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 8}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,291] Trial 42 finished with value: 0.3333333333333333 and parameters: {'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 8}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,300] Trial 43 finished with value: 0.4095238095238095 and parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 16}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,314] Trial 44 finished with value: 0.3523809523809524 and parameters: {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 15}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,325] Trial 45 finished with value: 0.38095238095238093 and parameters: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 32}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,345] Trial 46 finished with value: 0.3761904761904762 and parameters: {'criterion': 'gini', 'max_depth': 8, 'min_samples_split': 22}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,362] Trial 47 finished with value: 0.3761904761904762 and parameters: {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 18}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,373] Trial 48 finished with value: 0.32857142857142857 and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 56}. Best is trial 21 with value: 0.4095238095238095.\n",
      "[I 2025-03-02 21:57:59,382] Trial 49 finished with value: 0.3523809523809524 and parameters: {'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 2}. Best is trial 21 with value: 0.4095238095238095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 17}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "def fill_null_mode(df):\n",
    "    mode = df.mode().iloc[0]\n",
    "    return df.fillna(mode)\n",
    "\n",
    "df_train['Attendance (%)'] = df_train['Attendance (%)'].fillna(df_train['Attendance (%)'].mean())\n",
    "df_train['Assignments_Avg'] = df_train['Assignments_Avg'].fillna(df_train['Assignments_Avg'].mean())\n",
    "df_train['Parent_Education_Level'] = fill_null_mode(df_train['Parent_Education_Level'])\n",
    "\n",
    "df_test['Attendance (%)'] = df_test['Attendance (%)'].fillna(df_test['Attendance (%)'].mean())\n",
    "df_test['Assignments_Avg'] = df_test['Assignments_Avg'].fillna(df_test['Assignments_Avg'].mean())\n",
    "df_test['Parent_Education_Level'] = fill_null_mode(df_test['Parent_Education_Level'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_train.drop(columns=['Email', 'Student_ID'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['Email', 'Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_columns = ['First_Name', 'Last_Name', 'Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Splitting data\n",
    "X = df_train.drop(columns=['Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy', 'log_loss'])\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n",
    "    \n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Predict on test data\n",
    "prediction = final_model.predict(df_test)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': prediction})\n",
    "df_submission.to_csv(\"submitlast.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "18680bcf-10d4-428e-b62b-7f43c78068f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 22:06:35,215] A new study created in memory with name: no-name-5f4c36f2-ef8a-4694-a53c-9064329774ea\n",
      "[I 2025-03-02 22:06:35,614] Trial 0 finished with value: 0.35714285714285715 and parameters: {'n_estimators': 252, 'max_depth': 48, 'min_samples_split': 4, 'min_samples_leaf': 4, 'bootstrap': False}. Best is trial 0 with value: 0.35714285714285715.\n",
      "[I 2025-03-02 22:06:35,772] Trial 1 finished with value: 0.3476190476190476 and parameters: {'n_estimators': 156, 'max_depth': 25, 'min_samples_split': 17, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 0 with value: 0.35714285714285715.\n",
      "[I 2025-03-02 22:06:36,069] Trial 2 finished with value: 0.3333333333333333 and parameters: {'n_estimators': 232, 'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 1, 'bootstrap': True}. Best is trial 0 with value: 0.35714285714285715.\n",
      "[I 2025-03-02 22:06:36,350] Trial 3 finished with value: 0.3952380952380952 and parameters: {'n_estimators': 191, 'max_depth': 43, 'min_samples_split': 14, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:36,484] Trial 4 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 128, 'max_depth': 6, 'min_samples_split': 19, 'min_samples_leaf': 1, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:36,594] Trial 5 finished with value: 0.3523809523809524 and parameters: {'n_estimators': 100, 'max_depth': 49, 'min_samples_split': 10, 'min_samples_leaf': 4, 'bootstrap': True}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:36,721] Trial 6 finished with value: 0.37142857142857144 and parameters: {'n_estimators': 120, 'max_depth': 7, 'min_samples_split': 20, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:36,894] Trial 7 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 165, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 5, 'bootstrap': True}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:37,141] Trial 8 finished with value: 0.3619047619047619 and parameters: {'n_estimators': 187, 'max_depth': 19, 'min_samples_split': 14, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:37,333] Trial 9 finished with value: 0.38571428571428573 and parameters: {'n_estimators': 192, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:37,404] Trial 10 finished with value: 0.3619047619047619 and parameters: {'n_estimators': 52, 'max_depth': 37, 'min_samples_split': 14, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:37,684] Trial 11 finished with value: 0.3619047619047619 and parameters: {'n_estimators': 211, 'max_depth': 39, 'min_samples_split': 8, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:38,079] Trial 12 finished with value: 0.37142857142857144 and parameters: {'n_estimators': 296, 'max_depth': 37, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:38,432] Trial 13 finished with value: 0.3523809523809524 and parameters: {'n_estimators': 260, 'max_depth': 14, 'min_samples_split': 16, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:38,700] Trial 14 finished with value: 0.3380952380952381 and parameters: {'n_estimators': 198, 'max_depth': 32, 'min_samples_split': 7, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:38,895] Trial 15 finished with value: 0.38095238095238093 and parameters: {'n_estimators': 149, 'max_depth': 14, 'min_samples_split': 11, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:39,203] Trial 16 finished with value: 0.35714285714285715 and parameters: {'n_estimators': 219, 'max_depth': 45, 'min_samples_split': 14, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:39,436] Trial 17 finished with value: 0.3952380952380952 and parameters: {'n_estimators': 181, 'max_depth': 31, 'min_samples_split': 5, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:39,509] Trial 18 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 71, 'max_depth': 42, 'min_samples_split': 5, 'min_samples_leaf': 9, 'bootstrap': True}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:39,881] Trial 19 finished with value: 0.3761904761904762 and parameters: {'n_estimators': 297, 'max_depth': 32, 'min_samples_split': 2, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:40,272] Trial 20 finished with value: 0.3523809523809524 and parameters: {'n_estimators': 242, 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:40,519] Trial 21 finished with value: 0.38095238095238093 and parameters: {'n_estimators': 182, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 7, 'bootstrap': False}. Best is trial 3 with value: 0.3952380952380952.\n",
      "[I 2025-03-02 22:06:40,777] Trial 22 finished with value: 0.4 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:40,959] Trial 23 finished with value: 0.3904761904761905 and parameters: {'n_estimators': 139, 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:41,258] Trial 24 finished with value: 0.3904761904761905 and parameters: {'n_estimators': 219, 'max_depth': 29, 'min_samples_split': 16, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:41,495] Trial 25 finished with value: 0.37142857142857144 and parameters: {'n_estimators': 174, 'max_depth': 35, 'min_samples_split': 13, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:41,748] Trial 26 finished with value: 0.3476190476190476 and parameters: {'n_estimators': 272, 'max_depth': 43, 'min_samples_split': 18, 'min_samples_leaf': 10, 'bootstrap': True}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:42,028] Trial 27 finished with value: 0.3476190476190476 and parameters: {'n_estimators': 203, 'max_depth': 24, 'min_samples_split': 15, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:42,254] Trial 28 finished with value: 0.38571428571428573 and parameters: {'n_estimators': 168, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:42,430] Trial 29 finished with value: 0.34285714285714286 and parameters: {'n_estimators': 107, 'max_depth': 47, 'min_samples_split': 5, 'min_samples_leaf': 3, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:42,742] Trial 30 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 236, 'max_depth': 28, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:42,920] Trial 31 finished with value: 0.38095238095238093 and parameters: {'n_estimators': 135, 'max_depth': 23, 'min_samples_split': 12, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,107] Trial 32 finished with value: 0.38571428571428573 and parameters: {'n_estimators': 141, 'max_depth': 17, 'min_samples_split': 13, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,313] Trial 33 finished with value: 0.37142857142857144 and parameters: {'n_estimators': 158, 'max_depth': 22, 'min_samples_split': 11, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,492] Trial 34 finished with value: 0.32857142857142857 and parameters: {'n_estimators': 178, 'max_depth': 26, 'min_samples_split': 17, 'min_samples_leaf': 8, 'bootstrap': True}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,673] Trial 35 finished with value: 0.3619047619047619 and parameters: {'n_estimators': 115, 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,768] Trial 36 finished with value: 0.37142857142857144 and parameters: {'n_estimators': 91, 'max_depth': 21, 'min_samples_split': 15, 'min_samples_leaf': 10, 'bootstrap': True}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:43,984] Trial 37 finished with value: 0.3476190476190476 and parameters: {'n_estimators': 149, 'max_depth': 50, 'min_samples_split': 9, 'min_samples_leaf': 6, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:44,289] Trial 38 finished with value: 0.35714285714285715 and parameters: {'n_estimators': 228, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 2, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:44,492] Trial 39 finished with value: 0.34285714285714286 and parameters: {'n_estimators': 201, 'max_depth': 26, 'min_samples_split': 11, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:44,681] Trial 40 finished with value: 0.3523809523809524 and parameters: {'n_estimators': 126, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 5, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:44,957] Trial 41 finished with value: 0.3904761904761905 and parameters: {'n_estimators': 215, 'max_depth': 29, 'min_samples_split': 16, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:45,215] Trial 42 finished with value: 0.3476190476190476 and parameters: {'n_estimators': 191, 'max_depth': 29, 'min_samples_split': 19, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:45,451] Trial 43 finished with value: 0.36666666666666664 and parameters: {'n_estimators': 168, 'max_depth': 35, 'min_samples_split': 16, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:45,733] Trial 44 finished with value: 0.38095238095238093 and parameters: {'n_estimators': 224, 'max_depth': 25, 'min_samples_split': 15, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:46,074] Trial 45 finished with value: 0.3619047619047619 and parameters: {'n_estimators': 254, 'max_depth': 16, 'min_samples_split': 17, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:46,325] Trial 46 finished with value: 0.35714285714285715 and parameters: {'n_estimators': 184, 'max_depth': 21, 'min_samples_split': 14, 'min_samples_leaf': 10, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:46,641] Trial 47 finished with value: 0.4 and parameters: {'n_estimators': 241, 'max_depth': 34, 'min_samples_split': 13, 'min_samples_leaf': 9, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:46,968] Trial 48 finished with value: 0.3761904761904762 and parameters: {'n_estimators': 244, 'max_depth': 39, 'min_samples_split': 12, 'min_samples_leaf': 8, 'bootstrap': False}. Best is trial 22 with value: 0.4.\n",
      "[I 2025-03-02 22:06:47,237] Trial 49 finished with value: 0.3380952380952381 and parameters: {'n_estimators': 265, 'max_depth': 34, 'min_samples_split': 13, 'min_samples_leaf': 7, 'bootstrap': True}. Best is trial 22 with value: 0.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 12, 'min_samples_leaf': 9, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "df_train = pd.read_csv(\"train_data.csv\")\n",
    "df_test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "def fill_null_mode(df):\n",
    "    mode = df.mode().iloc[0]\n",
    "    return df.fillna(mode)\n",
    "\n",
    "df_train['Attendance (%)'] = df_train['Attendance (%)'].fillna(df_train['Attendance (%)'].mean())\n",
    "df_train['Assignments_Avg'] = df_train['Assignments_Avg'].fillna(df_train['Assignments_Avg'].mean())\n",
    "df_train['Parent_Education_Level'] = fill_null_mode(df_train['Parent_Education_Level'])\n",
    "\n",
    "df_test['Attendance (%)'] = df_test['Attendance (%)'].fillna(df_test['Attendance (%)'].mean())\n",
    "df_test['Assignments_Avg'] = df_test['Assignments_Avg'].fillna(df_test['Assignments_Avg'].mean())\n",
    "df_test['Parent_Education_Level'] = fill_null_mode(df_test['Parent_Education_Level'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_train.drop(columns=['First_Name', 'Last_Name', 'Email', 'Student_ID'], inplace=True)\n",
    "student_id = df_test['Student_ID']\n",
    "df_test.drop(columns=['First_Name', 'Last_Name', 'Email', 'Student_ID'], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_columns = ['Gender', 'Department', 'Extracurricular_Activities', 'Internet_Access_at_Home', 'Parent_Education_Level', 'Family_Income_Level']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col])\n",
    "    df_test[col] = le.transform(df_test[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Splitting data\n",
    "X = df_train.drop(columns=['Grade'])\n",
    "y = df_train['Grade']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define Optuna objective function\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Predict on test data\n",
    "prediction = final_model.predict(df_test)\n",
    "\n",
    "# Save results\n",
    "df_submission = pd.DataFrame({'Student_ID': student_id, 'Grade': prediction})\n",
    "df_submission.to_csv(\"sample_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f0189-4715-45a6-b7ce-810fc1e60d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
